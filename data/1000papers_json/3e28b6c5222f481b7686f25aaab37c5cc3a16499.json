{"paper_id": "3e28b6c5222f481b7686f25aaab37c5cc3a16499", "metadata": {"title": "Extraction and Recognition of Bangla Texts from Natural Scene Images Using CNN", "authors": [{"first": "Rashedul", "middle": [], "last": "Islam", "suffix": "", "affiliation": {"laboratory": "", "institution": "Khulna University", "location": {"postCode": "9208", "settlement": "Khulna", "country": "Bangladesh"}}, "email": ""}, {"first": "Rafiqul", "middle": [], "last": "Islam", "suffix": "", "affiliation": {"laboratory": "", "institution": "Khulna University", "location": {"postCode": "9208", "settlement": "Khulna", "country": "Bangladesh"}}, "email": ""}, {"first": "Kamrul", "middle": [], "last": "Hasan Talukder", "suffix": "", "affiliation": {"laboratory": "", "institution": "Khulna University", "location": {"postCode": "9208", "settlement": "Khulna", "country": "Bangladesh"}}, "email": ""}]}, "abstract": [{"text": "The semantic information presents in the scene images may be the useful information for the viewers who is searching for a specific location or any specific shop and address. This type of information can also be useful in licenseplate detection, controlling the vehicle on the road, robot navigation, and assisting visually impaired persons. An efficient method is presented in this paper to detect and extract Bangla texts from scene images based on a connected component approach along with rule-based filtering and vertical scanning scheme. Next, extracted characters are recognized by using Convolutional Neural Network (CNN). The method consists of the four basic consecutive steps such as detection and extraction of the Region of Interest (ROI), segmentation of the words, extraction of characters, and recognition of the extracted characters. After extracting the ROI from the input image, connected component(CC) analysis and bounding box technology are used for segmentation of Bangla words. To separate and extract Bangla characters from the segmented Bangla words, vertical scanning based method along with a dynamic threshold value has been applied. Finally, character recognition is carried out using CNN. The proposed algorithm is applied to 600 scene images of different writing styles and colors, and we have obtained 89.25% accuracy in text detection and 94.50% accuracy in the extraction of characters. We have achieved an accuracy of 99.30% and 95.76% in recognition of Bangla digits and characters respectively. By combining both the digits and characters, obtained recognition accuracy is 95.39%.", "cite_spans": [], "ref_spans": [], "section": "Abstract"}], "body_text": [{"text": "It is always challenging as well as an important task to extract and recognize texts from natural scene images. These types of images include banners, posters, billboards, license plates, etc. which may contain valuable information. This type of information can be used in many applications like the text to speech conversion, text based image indexing, text mining [1] , robot navigation, license plate recognition [2, 3] etc. The variation in font size, color, style, alignment, light intensity, blurry image, noise, etc. makes it a difficult issue to design a standard Text Information Extraction (TIE) system. The extraction of Bangla text is another challenging issue as headline or 'matra' presents in this type of text. A 'matra' is a horizontal line located at the upper portion of a character. A Bangla text may be partitioned into three zones as shown in Fig. 1 . As Bangla characters are connected by a headline or 'matra', we have proposed and applied a new algorithm to separate characters from each of the Bangla words by the method of vertical projection along with dynamic threshold values. The whole process of character detection, extraction, and recognition has been described in Sect. 3 .", "cite_spans": [{"start": 366, "end": 369, "text": "[1]", "ref_id": "BIBREF0"}, {"start": 416, "end": 419, "text": "[2,", "ref_id": "BIBREF1"}, {"start": 420, "end": 422, "text": "3]", "ref_id": "BIBREF2"}, {"start": 1205, "end": 1206, "text": "3", "ref_id": "BIBREF2"}], "ref_spans": [{"start": 865, "end": 871, "text": "Fig. 1", "ref_id": "FIGREF0"}], "section": "Introduction"}, {"text": "There is no benchmark database of scene images containing Bangla texts to perform research on extraction and recognition of Bangla characters. From this point of view, we have contributed to this field by providing a database of scene images consisting of 600 images. Another contribution of this paper is that we have a rich collection of Bangla characters which can be used by other researchers in developing a system of searching and recognition of office documents, text in scene images, etc.", "cite_spans": [], "ref_spans": [], "section": "Introduction"}, {"text": "Text detection is a very challenging task for researchers who work with natural scene images. Various methods have been introduced earlier for the detection and localization of texts from scene images. In [2] [3] [4] , text detection and localization techniques have been discussed based on the edge, texture, CC, stroke, and different combination of these methods. An edge detector is used in edge based method [5] [6] [7] for detecting the edges followed by morphological operation.", "cite_spans": [{"start": 205, "end": 208, "text": "[2]", "ref_id": "BIBREF1"}, {"start": 209, "end": 212, "text": "[3]", "ref_id": "BIBREF2"}, {"start": 213, "end": 216, "text": "[4]", "ref_id": "BIBREF3"}, {"start": 412, "end": 415, "text": "[5]", "ref_id": "BIBREF4"}, {"start": 416, "end": 419, "text": "[6]", "ref_id": "BIBREF5"}, {"start": 420, "end": 423, "text": "[7]", "ref_id": "BIBREF6"}], "ref_spans": [], "section": "Related Work"}, {"text": "Bangla text extraction from the natural scene images is still now an ongoing research [8] . In the early stage, most of the researchers were concerned only with the images of printed documents, where the text was written in black color with white background [9] . Another method proposed by A. Asaduzzaman et al. [10] to detect and recognize Bangla text from printed documents using the heuristic method and Artificial Neural Network (ANN).", "cite_spans": [{"start": 86, "end": 89, "text": "[8]", "ref_id": "BIBREF7"}, {"start": 258, "end": 261, "text": "[9]", "ref_id": "BIBREF8"}, {"start": 313, "end": 317, "text": "[10]", "ref_id": "BIBREF9"}], "ref_spans": [], "section": "Related Work"}, {"text": "U. Bhattacharya et al. [11] , proposed a method for the recognition of Bangla characters from scene images. The method can separate the CCs from scene images using morphological operation by calculating height and standard deviation of the CCs. Their achieved precision and recall values were 68.8% and 71.2% respectively considering a set of 100 images. R. Ghoshal et al. [12] proposed a morphological approach for Bangla text extraction from images. Their approach was limited to highlighted texts only. The algorithm can perform detection of text area and segmentation of CCs.", "cite_spans": [{"start": 23, "end": 27, "text": "[11]", "ref_id": "BIBREF10"}, {"start": 373, "end": 377, "text": "[12]", "ref_id": "BIBREF11"}], "ref_spans": [], "section": "Related Work"}, {"text": "In [13] , a texture based method was proposed to detect text at gray level natural scene images. A probabilistic model with ANN based classifier is used here to separate text from non-text objects. They achieved text detection and false alarm rate of 64% and 25% respectively.", "cite_spans": [{"start": 3, "end": 7, "text": "[13]", "ref_id": "BIBREF12"}], "ref_spans": [], "section": "Related Work"}, {"text": "The detail description of the originality and other contributions of the work are given below. ", "cite_spans": [], "ref_spans": [], "section": "Related Work"}, {"text": "In this paper, the proposed method is executed in two phases. Such as: Character Extraction and Character Recognition. In the first phase, the main emphasis is given on text localization and extraction that lead to better accuracy of character extraction. In the second phase, character recognition is performed by using CNN. At first, the text area is selected then each of the text regions is marked by a rectangular bounding box and finally, individual characters are extracted from each text region using the newly proposed vertical scanning algorithm.", "cite_spans": [], "ref_spans": [], "section": "Proposed Method"}, {"text": "In this phase, a database of scene image is prepared. Then some pre-processing measures are taken to resize the images into 500 \u00d7 500 pixels. Then the images are converted to Binary image. Some other necessary steps are taken to extract the characters from the scene images. The detail description of each of the steps of this phase is stated below.", "cite_spans": [], "ref_spans": [], "section": "Character Extraction"}, {"text": "Since there is no benchmark database of scene images with Bangla text, we have collected scene images from different locations of Bangladesh using the digital camera and the camera of the Pre-processing: This step involves two subsections as mentioned below.", "cite_spans": [], "ref_spans": [], "section": "Preparing a Database of Scene Images:"}, {"text": "Convert to the Grayscale Image: The captured images are the RGB image. So, to prepare them for the next step, we have to convert them into grayscale images. We have done it by using the National Television Standard Committee (NTSC) standard as shown in (1).", "cite_spans": [], "ref_spans": [], "section": "Preparing a Database of Scene Images:"}, {"text": ".", "cite_spans": [], "ref_spans": [], "section": "Preparing a Database of Scene Images:"}, {"text": "To convert the grayscale image to a binary image, a threshold value is selected and all the gray level pixels below the threshold value are classified as 0 (black or background) and all the gray level pixels, equal to or greater than the threshold value are classified as 1(white or foreground) as shown in (2).", "cite_spans": [], "ref_spans": [], "section": "Convert to the Binary Image:"}, {"text": "Here, g(x, y) represents the threshold image pixel at (x, y) and f(x, y) represents grayscale image pixel at (x, y).", "cite_spans": [], "ref_spans": [], "section": "Convert to the Binary Image:"}, {"text": "In this process, the best possible regions are selected as ROI by the users. It helps to decreases false positive and also helps to collect more Bangla characters for preparing the training and test set.", "cite_spans": [], "ref_spans": [], "section": "Detection and Extraction of ROI:"}, {"text": "Word Segmentation: CC based approach along with bounding box technology is applied to select each of the Bangla words as CCs. For this purpose, we have used the labeling of CCs of the binary image. Here all the CCs are marked by the red color rectangular bounding boxes as shown in Fig. 2 (c).", "cite_spans": [], "ref_spans": [{"start": 282, "end": 288, "text": "Fig. 2", "ref_id": "FIGREF2"}], "section": "Detection and Extraction of ROI:"}, {"text": "Character Extraction: Bangla character extraction is one of the challenging tasks of the character recognition system. As the words are connected by a headline or 'matra', it is difficult to segment out individual characters. The technique to remove headlines to separate characters from Bangla words has been followed by the existing methods. But the main problem of removing 'matra' is that after removing the 'matra' some characters will be changed to another character. Some examples of such characters are shown in Fig. 3 . We can solve this problem by the following way. At first, we take all the CCs as input and count the number of white pixels in every column to determine minimum value among all the columns. The column that contains minimum value will be treated as a separating zone among the characters of a word. To separate two characters vertically, we have set the pixel values of all the pixels of a specific column to 0 where the number of white pixels of the column is less than (minimum+5). Then the separated characters are resized to 16 \u00d7 16 pixels and store them into a specific folder as Bangla characters. ", "cite_spans": [], "ref_spans": [{"start": 520, "end": 526, "text": "Fig. 3", "ref_id": "FIGREF3"}], "section": "Detection and Extraction of ROI:"}, {"text": "This is the final stage of the proposed method. In this stage, experiment is performed based on the two consecutive phases such as the training and the testing phase. The brief description of each of the phases is stated below:", "cite_spans": [], "ref_spans": [], "section": "Character Recognition"}, {"text": "Prepare Training and Test Dataset: To prepare the data sets, at first it is required to load the database named 'banglacharacter' as an imagedatastore. The main function of the imagedatastore object is to automatically labels the images based on folder names. Finally data are stored as an imagedatastore object. To prepare the training data set, the system will randomly select a fixed number of images as mentioned by the user from each of the folders containing Bangla characters.", "cite_spans": [], "ref_spans": [], "section": "Training Phase"}, {"text": "In this experiment, we have assigned 250 as the number of images to be selected from each folder for training. The remaining characters of the folder will be treated as a test data set.", "cite_spans": [], "ref_spans": [], "section": "Training Phase"}, {"text": "Initialize the CNN Layers: CNN is designed with many layers. To work with CNN, at first we have to define each of the layers by specific parameter values. Brief description of the layers of our designed CNN is stated below:", "cite_spans": [], "ref_spans": [], "section": "Training Phase"}, {"text": "-Image Input Layer: In this layer, the image size is specified for our database. We have specified the said size as 16-by-16-by-1. Here, height and width of the image is 16 and the channel size is 1. The 'banglacharacter' data consists of binary images, so the channel size is 1. For a color image, the channel size 3 is recommended. -Convolutional Layer: This layer contains three parameters. The first parameter is the filter size. The second parameter is the number of filters, which represents the number of neurons that connect to the same region of the input. 'Padding' name-value pair is used to add padding to the input feature map. We have used the following hyperparameters for the function convolu-tion2dLayer ( Train the Network: The Main purpose of training is to perform the task of recognition successfully. For this, the training data set is used along with predefined values of CNN layers and training options. These three parameters help to train the CNN successfully.", "cite_spans": [], "ref_spans": [], "section": "Training Phase"}, {"text": "Classify Using the Trained CNN: In this step, all the characters under the test data set are classified using the trained CNN. In this process labels of test data set are matched with the labels of the training data and obtained result is stored as predicted data.", "cite_spans": [], "ref_spans": [], "section": "Testing Phase"}, {"text": "Calculation of Accuracy: At first, labels of test data set are stored as test validation. Then Recognition accuracy is calculated by making a one-to-one comparison between the predicted data and the test validation. Figure 5 shows the system architecture of the proposed method. ", "cite_spans": [], "ref_spans": [{"start": 216, "end": 224, "text": "Figure 5", "ref_id": "FIGREF6"}], "section": "Testing Phase"}, {"text": "The experiments were conducted in the following two phases. Such as a) Character extraction and b) Calculation of the accuracy of recognition.", "cite_spans": [], "ref_spans": [], "section": "Experimental Results"}, {"text": "In the first phase, Bangla characters were extracted from the natural scene images and in the second phase training and testing were performed on the extracted characters and the accuracy of recognition was calculated. All the experiments were performed in MATLAB environment using the images of our image database. The proposed method was applied to 600 scene images. The algorithm will not work properly or fail in the case of character extraction if all the characters are connected with each other by any way other than \"matra\". The Algorithm will fail in another case where the texts are written in a curved or round shape. A few such images are shown in Fig. 6 where the proposed algorithm will fail to extract Bangla characters. Though there are some limitations of the proposed method, it is better in comparison with the existing methods regarding the results of the accuracy of extraction and the results of the accuracy of the character recognition. Detail description of the major two phases of the experimental results is given below.", "cite_spans": [], "ref_spans": [{"start": 660, "end": 666, "text": "Fig. 6", "ref_id": "FIGREF7"}], "section": "Experimental Results"}, {"text": "To analyze the results of character extraction, we have used four metrics, such as precision, recall, f1-score, and accuracy based on the following parameters [14] . True Positive (TP), True Negative (TN), False Positive (FP), and False Negative (FN). The accuracy of character extraction is calculated by the way as shown in (4) . Table 1 shows the percentage of precision, recall, f1-score, and the accuracy of character extractions from different types of scene images like banners, posters and license plates. ", "cite_spans": [{"start": 159, "end": 163, "text": "[14]", "ref_id": "BIBREF13"}, {"start": 326, "end": 329, "text": "(4)", "ref_id": "BIBREF3"}], "ref_spans": [{"start": 332, "end": 339, "text": "Table 1", "ref_id": "TABREF0"}], "section": "Calculation of the Accuracy of Character Extraction"}, {"text": "To calculate the accuracy of recognition, CNN predicts the labels of the test data using the trained network, and calculate the final validation accuracy. Accuracy is the fraction of labels that the network predicts correctly. Recognition accuracy is calculated by the following equation as shown in (5).", "cite_spans": [], "ref_spans": [], "section": "Calculation of the Accuracy of Recognition"}, {"text": "Accuracy = total number of matching labels total number of elements in the test data \u00d7 100%", "cite_spans": [], "ref_spans": [], "section": "Calculation of the Accuracy of Recognition"}, {"text": "The comparison of recognition accuracy is shown in Table 2 . The cited approaches mentioned in Table 2 do not use the same database as ours. In the table, '-' indicates that the result was not found in the respective paper. From Table 2 , it is clear that the proposed method outperforms the existing methods. ", "cite_spans": [], "ref_spans": [{"start": 51, "end": 58, "text": "Table 2", "ref_id": "TABREF1"}, {"start": 95, "end": 102, "text": "Table 2", "ref_id": "TABREF1"}, {"start": 229, "end": 236, "text": "Table 2", "ref_id": "TABREF1"}], "section": "Calculation of the Accuracy of Recognition"}, {"text": "The proposed method of Bangla character recognition has been tested on Bangla digits and letters extracted from the varied sorts of scene images and achieved smart ends up in comparison with the present strategies. To separate Bangla characters from the words, we have applied the vertical scanning algorithm. In the case of the extraction of Bangla characters, we've achieved 94.50% accuracy from 600 natural scene images. Within the recognition phase, character recognition is performed exploitation CNN classifier. We've used the CNN for the popularity due to its high accuracy. A hierarchical model is followed in the CNN that works on building a network, like a funnel, and at last offers out a fully-connected layer wherever all the neurons are connected and the output is processed. The achieved recognition accuracy for Bangla digits is 99.30% and for Bangla characters, it is 95.76% and their combined result is 95.39% that is best than the results of the present strategies. Our future set up is to counterpoint our information with all the essential characters and joined letters of the Bangla alphabet and to represent the recognized characters in the editable form.", "cite_spans": [], "ref_spans": [], "section": "Conclusions"}], "bib_entries": {"BIBREF0": {"ref_id": "b0", "title": "Efficiently mining frequent itemsets applied for textual aggregation", "authors": [{"first": "M", "middle": [], "last": "Bouakkaz", "suffix": ""}, {"first": "Y", "middle": [], "last": "Ouinten", "suffix": ""}, {"first": "S", "middle": [], "last": "Loudcher", "suffix": ""}, {"first": "P", "middle": [], "last": "Fournier-Viger", "suffix": ""}], "year": 2017, "venue": "Appl. Intell", "volume": "48", "issn": "4", "pages": "1013--1019", "other_ids": {"DOI": ["10.1007/s10489-017-1050-9"]}}, "BIBREF1": {"ref_id": "b1", "title": "Scene text detection and recognition: recent advances and future trends", "authors": [{"first": "Y", "middle": [], "last": "Zhu", "suffix": ""}, {"first": "C", "middle": [], "last": "Yao", "suffix": ""}, {"first": "X", "middle": [], "last": "Bai", "suffix": ""}], "year": 2016, "venue": "Front. Comput. Sci", "volume": "10", "issn": "1", "pages": "19--36", "other_ids": {}}, "BIBREF2": {"ref_id": "b2", "title": "Text extraction from natural scene image: a survey", "authors": [{"first": "H", "middle": [], "last": "Zhang", "suffix": ""}, {"first": "K", "middle": [], "last": "Zhao", "suffix": ""}, {"first": "Y", "middle": ["Z"], "last": "Song", "suffix": ""}, {"first": "J", "middle": [], "last": "Guo", "suffix": ""}], "year": 2013, "venue": "Neurocomputing", "volume": "122", "issn": "", "pages": "310--323", "other_ids": {}}, "BIBREF3": {"ref_id": "b3", "title": "A study on text detection and localization techniques for natural scene images", "authors": [{"first": "S", "middle": [], "last": "Unar", "suffix": ""}, {"first": "A", "middle": [], "last": "Hussain", "suffix": ""}, {"first": "M", "middle": [], "last": "Shaikh", "suffix": ""}, {"first": "K", "middle": ["H"], "last": "Memon", "suffix": ""}, {"first": "M", "middle": ["A"], "last": "Ansari", "suffix": ""}, {"first": "Z", "middle": [], "last": "Memon", "suffix": ""}], "year": 2018, "venue": "IJCSNS", "volume": "18", "issn": "1", "pages": "", "other_ids": {}}, "BIBREF4": {"ref_id": "b4", "title": "Scene text localization using edge analysis and feature pool", "authors": [{"first": "C", "middle": [], "last": "Yu", "suffix": ""}, {"first": "Y", "middle": [], "last": "Song", "suffix": ""}, {"first": "Y", "middle": [], "last": "Zhang", "suffix": ""}], "year": 2016, "venue": "Neurocomputing", "volume": "175", "issn": "", "pages": "652--661", "other_ids": {}}, "BIBREF5": {"ref_id": "b5", "title": "Edge detection and confidence map applied to identify textual elements in the image", "authors": [{"first": "B", "middle": ["L S"], "last": "Silva", "suffix": ""}, {"first": "P", "middle": ["M"], "last": "Ciarelli", "suffix": ""}], "year": 2016, "venue": "", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF6": {"ref_id": "b6", "title": "Scene text extraction with edge constraint and text collinearity", "authors": [{"first": "S", "middle": [], "last": "Lee", "suffix": ""}, {"first": "M", "middle": ["S"], "last": "Cho", "suffix": ""}, {"first": "K", "middle": [], "last": "Jung", "suffix": ""}, {"first": "J", "middle": ["H"], "last": "Kim", "suffix": ""}], "year": 2010, "venue": "20th International Conference on Pattern Recognition (ICPR)", "volume": "", "issn": "", "pages": "3983--3986", "other_ids": {}}, "BIBREF7": {"ref_id": "b7", "title": "Bangla text extraction from natural scene images for mobile applications", "authors": [{"first": "M", "middle": ["A"], "last": "Moyeen", "suffix": ""}, {"first": "K", "middle": ["M R"], "last": "Alam", "suffix": ""}, {"first": "M", "middle": ["A"], "last": "Awal", "suffix": ""}], "year": 2013, "venue": "J. Electr. Eng. Inst. Eng. EE", "volume": "39", "issn": "", "pages": "", "other_ids": {}}, "BIBREF8": {"ref_id": "b8", "title": "Non linear Gaussian filters performing edge preserving diffusion", "authors": [{"first": "V", "middle": [], "last": "Aurich", "suffix": ""}, {"first": "J", "middle": [], "last": "Weule", "suffix": ""}], "year": 1995, "venue": "17 DAGM Symposium", "volume": "", "issn": "", "pages": "538--545", "other_ids": {}}, "BIBREF9": {"ref_id": "b9", "title": "Printed Bangla text recognition using artificial neural network with heuristic method", "authors": [{"first": "A", "middle": [], "last": "Asaduzzaman", "suffix": ""}, {"first": "M", "middle": ["K I"], "last": "Molla", "suffix": ""}, {"first": "M", "middle": ["G"], "last": "Ali", "suffix": ""}], "year": 2002, "venue": "Proceedings of ICCIT", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF10": {"ref_id": "b10", "title": "Devanagari and Bangla text extraction from natural scene images", "authors": [{"first": "U", "middle": [], "last": "Bhattacharya", "suffix": ""}, {"first": "S", "middle": ["K"], "last": "Parui", "suffix": ""}, {"first": "S", "middle": [], "last": "Mondal", "suffix": ""}], "year": 2009, "venue": "Proceedings of International Conference on Document Analysis and Recognition", "volume": "", "issn": "", "pages": "26--29", "other_ids": {}}, "BIBREF11": {"ref_id": "b11", "title": "Headline based text extraction from outdoor images", "authors": [{"first": "R", "middle": [], "last": "Ghoshal", "suffix": ""}, {"first": "A", "middle": [], "last": "Roy", "suffix": ""}, {"first": "T", "middle": ["K"], "last": "Bhowmik", "suffix": ""}, {"first": "S", "middle": ["K"], "last": "Parui", "suffix": ""}], "year": 2011, "venue": "PReMI 2011", "volume": "6744", "issn": "", "pages": "446--451", "other_ids": {"DOI": ["10.1007/978-3-642-21786-9_72"]}}, "BIBREF12": {"ref_id": "b12", "title": "Texture based text detection in natural scene images: a help to blind and visually impaired persons", "authors": [{"first": "S", "middle": ["M"], "last": "Hanif", "suffix": ""}, {"first": "L", "middle": [], "last": "Prevost", "suffix": ""}], "year": null, "venue": "Conference Workshop on Assistive Technologies for People with Vision Hearing Impairments Assistive Technology for All Ages CVHI", "volume": "", "issn": "", "pages": "", "other_ids": {}}, "BIBREF13": {"ref_id": "b13", "title": "Histograms of oriented gradients for human detection", "authors": [{"first": "N", "middle": [], "last": "Dalal", "suffix": ""}, {"first": "B", "middle": [], "last": "Triggs", "suffix": ""}], "year": 2005, "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition", "volume": "1", "issn": "", "pages": "886--893", "other_ids": {}}, "BIBREF14": {"ref_id": "b14", "title": "Recognition of Bangla text from scene images through perspective correction", "authors": [{"first": "R", "middle": [], "last": "Ghoshal", "suffix": ""}, {"first": "A", "middle": [], "last": "Roy", "suffix": ""}, {"first": "S", "middle": ["K"], "last": "Parui", "suffix": ""}], "year": 2011, "venue": "2011 International Conference on Image Information Processing (ICIIP)", "volume": "", "issn": "", "pages": "1--6", "other_ids": {}}, "BIBREF15": {"ref_id": "b15", "title": "Recognition of Bangla text from outdoor images using decision tree model", "authors": [{"first": "R", "middle": [], "last": "Ghoshal", "suffix": ""}, {"first": "A", "middle": [], "last": "Roy", "suffix": ""}, {"first": "B", "middle": ["C"], "last": "Dhara", "suffix": ""}, {"first": "S", "middle": ["K"], "last": "Parui", "suffix": ""}], "year": 2017, "venue": "Int. J. Knowl.-Based Intell. Eng. Syst", "volume": "21", "issn": "1", "pages": "29--38", "other_ids": {}}}, "ref_entries": {"FIGREF0": {"text": "Different zones of Bangla text.", "latex": null, "type": "figure"}, "FIGREF1": {"text": "The system architecture has been proposed for the extraction and recognition of Bangla characters from scene images. This proposed architecture is a new one and got very good results both in extraction and recognition of Bangla characters. 2. The proposed method of character extraction is a new one and can effectively perform the task and gives better results than the existing methods. 3. The experimental results of character extraction and recognition have been compared with other related methods. The results show that our proposed method gives better results both in extraction and recognition of Bangla characters.", "latex": null, "type": "figure"}, "FIGREF2": {"text": "(a) Original color image (b) Binary image (c) Localization of Bangla words.", "latex": null, "type": "figure"}, "FIGREF3": {"text": "Effects of removing matra.", "latex": null, "type": "figure"}, "FIGREF4": {"text": "Batch Normalization Layer: This layer of CNN helps to speed up network training and reduces the sensitivity to network initialization. -ReLU Layer: A ReLU layer performs a threshold operation to each element of the input, where any value less than zero is set to zero as shown in 3. Max Pooling Layer: The max pooling layer returns the maximum values of rectangular regions of inputs. the hyperparameters that we have used for this layer are as follows: PoolSize: [2 2], Stride: [2 2], PaddingMode: 'manual', PaddingSize: [0 0 0 0] -Fully Connected Layer: The last fully connected layer combines the features to classify the images. This layer is equal to the number of classes in the target data. For the classification of Bangla digits, we have used the function fullyConnectedLayer(10) with the two following hyperparameters. InputSize: 'auto' and OutputSize: shows the working process of a CNN with input image 'zero'.", "latex": null, "type": "figure"}, "FIGREF5": {"text": "Classification process of CNN. Set the Training Options: Before train the CNN classifier, it is required to specify the training options for classification of Bangla digits and characters. The Following training options have been used in this experiment. options = trainingOptions('sgdm', 'InitialLearnRate', 0.01, 'MaxEpochs', 4, 'ValidationData', imdsValidation, 'ValidationFrequency', 30, 'Verbose', false, 'Plots', 'training-progress').", "latex": null, "type": "figure"}, "FIGREF6": {"text": "System architecture of the proposed method.", "latex": null, "type": "figure"}, "FIGREF7": {"text": "Images where the proposed algorithm will fail.", "latex": null, "type": "figure"}, "TABREF0": {"text": "Results of character extraction.", "latex": null, "type": "table", "html": "<html><body><table><tr><td>Image type </td><td>No. of images </td><td>Pr(%) </td><td>RR(%) </td><td>F1 score(%) </td><td>Accuracy(%)\n</td></tr><tr><td>Banner </td><td>240 </td><td>92.73 </td><td>94.08 </td><td>93.00 </td><td>92.92\n</td></tr><tr><td>Poster </td><td>260 </td><td>95.30 </td><td>94.13 </td><td>94.45 </td><td>95.09\n</td></tr><tr><td>License plate </td><td>100 </td><td>95.43 </td><td>97.58 </td><td>96.32 </td><td>95.48\n</td></tr></table></body></html>"}, "TABREF1": {"text": "Comparison of the recognition accuracy with existing methods.", "latex": null, "type": "table", "html": "<html><body><table><tr><td>Methods </td><td>No. of images </td><td>No. of characters </td><td>Recognition accuracy(%)\n</td></tr><tr><td>Proposed </td><td>600 </td><td>21108 </td><td>95.39\n</td></tr><tr><td>Moyeen, M.A., et al. [8] </td><td>400 </td><td>-- </td><td>73.25\n</td></tr><tr><td>Ghoshal, R., et al. [15] </td><td>100 </td><td>7500 </td><td>85.93\n</td></tr><tr><td>Ghoshal, R., et al. [16] </td><td>250 </td><td>7100 </td><td>92.00\n</td></tr></table></body></html>"}}, "back_matter": []}